{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Web Scraping for Indeed.com and Predicting Salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "focus": false,
    "id": "2efefc73-064a-482d-b3b5-ddf5508cb4ec"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.indeed.com/jobs?q=data%20scientist%20%2420%2C000&l=New%20York&start=10%22&vjk=b9dedd85514851bc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "focus": false,
    "id": "2c6752c4-7704-4c94-8bc0-6f13d2d0d570"
   },
   "outputs": [],
   "source": [
    "# Proxy setup\n",
    "\n",
    "proxy = {\n",
    "        'https': \"stuartlee165%40outlook.com:Rockwood123@91.189.176.133:443/\",\n",
    "        'http': \"stuartlee165%40outlook.com:Rockwood123@91.189.176.133:443/\"\n",
    "}\n",
    "\n",
    "URL = \"https://www.indeed.com/jobs?q=data%20scientist%20%2420%2C000&l=Holtsville&start=10%22&vjk=b9dedd85514851bc\"\n",
    "r = requests.get(URL, proxies = proxy)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(\n",
    "    \"https://httpbin.org/get\",\n",
    "    proxies={\n",
    "        \"http\": \"http://116df877246d49fc9dd96b8efd4787e3:@proxy.crawlera.com:8011/\",\n",
    "        \"https\": \"http://116df877246d49fc9dd96b8efd4787e3:@proxy.crawlera.com:8011/\",\n",
    "    },\n",
    "    verify='/path/to/zyte-proxy-ca.crt' \n",
    ")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My initial strategy was to get a list of all the USA cities and a list of data science job titles from some CSVs I find online (this turned out to be very inefficient). Below code is me cleaning these lists up. Dropping duplicates.\n",
    "addresses = pd.read_csv('/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-4/us_cities_states_counties.csv', sep='|', index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18857,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities = addresses.City\n",
    "cities = cities.drop_duplicates()\n",
    "cities.head(5)\n",
    "cities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = pd.read_excel('/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-4/jobs_summary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_dscience = job_titles[job_titles.Category == 'Data Science']['Job Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles_dscience.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "job_titles_dscience.loc[195] = 'scientist data'\n",
    "job_titles_dscience.loc[196] = 'principal data scientist'\n",
    "job_titles_dscience.loc[199] = 'decision data scientist'\n",
    "job_titles_dscience.loc[202] = 'staff data scientist'\n",
    "job_titles_dscience.loc[204] = 'lead data scientist'\n",
    "job_titles_dscience.loc[212] = 'positive bioscience data'\n",
    "job_titles_dscience.loc[217] = 'chief data science officer'\n",
    "job_titles_dscience.loc[218] = 'president chief data scientist'\n",
    "job_titles_dscience.loc[205] = 'analytic data science scientist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.str.replace(' ', '%20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_dscience = job_titles_dscience.str.replace(' ', '%20')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed a function to estimate how many pages to search for under each city and job title. This is called later in my main scrape code. \n",
    "\n",
    "E.g. Chicago - data scientist would have many more pages to loop through than Wyoming - data intern (probably). Basically this finds the 'jobs found = 1000' item on the Indeed page and based on the fact that most pages list about 18 jobs calculates that it should iterate though 1000/18 times. Regex is used to filter out the '1000' number from the other bits of text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that estimates number of pages to iterate through per search result\n",
    "\n",
    "def pages_estimate(url):\n",
    "    try:\n",
    "        r = requests.get(url, proxies=proxies, verify='/Users/stuart/Downloads/zyte-smartproxy-ca.crt')\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # Return number of pages on search result:\n",
    "        job_found_text = soup.find('div', attrs={'id':'searchCountPages'}).text\n",
    "        pattern = re.compile(r'(?<=of.)\\d+ |(?<=of.)\\d,\\d+') ## filter so only shows integer\n",
    "        number_text = re.findall(pattern, job_found_text)\n",
    "        number_text = number_text[0].replace(\",\",\"\")\n",
    "        jobs_found = int(number_text)\n",
    "        pages_to_loop = (round(jobs_found /18)*10)+10 # Set number of pages to loop for\n",
    "    except:\n",
    "        pages_to_loop = 20\n",
    "    return pages_to_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_estimate('https://www.indeed.com/jobs?q=data%20scientist&l=New%20York&start=0&vjk=b9dedd85514851bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities2 = ['Chicago', 'New%20York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities3 = ['New%20York', 'Chicago', 'San%20Francisco', 'Austin', 'Seattle', \n",
    "    'Los%20Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "    'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities4= ['nashville', 'washington', 'Oklahoma', 'Seattle', 'Texas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities5= ['london']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_dscience2 = ['data%20science', 'data%20engineer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main code used for the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "focus": false,
    "id": "a1af53c9-9090-494f-b82e-cadb60a54909",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [36:01<00:00, 432.22s/it]\n",
      "100%|██████████| 5/5 [14:06<00:00, 169.32s/it]\n"
     ]
    }
   ],
   "source": [
    "########################## SETS UP SOME EMPTY LISTS ####################\n",
    "location = []\n",
    "company = []\n",
    "job_title = []\n",
    "salary = []\n",
    "city_search = []\n",
    "title_search = []\n",
    "link = []\n",
    "job_desc = []\n",
    "date_since_post = []\n",
    "company_rating = []\n",
    "\n",
    "### Load count file which stores information on how far through loop python was on last run\n",
    "\n",
    "\n",
    "########################## FILES ARE OUTPUTTED TO CSVS, \n",
    "########################## TRY: JUST CHECKS IF THIS IS THE FIRST TIME THE MODEL IS RUN\n",
    "########################## -IF THE MODEL HAS BEEN RUN BEFORE THEN IT IMPORTS THE CSVS WHERE NEEDED\n",
    "########################## EXCEPT: OR WHETHER IT HAS BEEN RUN BEFORE\n",
    "########################## - IF THE MODEL HASNT BEEN RUN BEFORE IT CREATES NEW EMPTY CSVS TO BE USED LATER IN CODE\n",
    "\n",
    "try: # try and load previous CSV files \n",
    "    counts = pd.read_csv('/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-4/count_file.csv', sep=',', index_col=False) \n",
    "    j_title_count = counts.j_title1[0]\n",
    "    cities_count = counts.cities_count1[0]\n",
    "    \n",
    "    stats = pd.read_csv('/Users/stuart/Desktop/GA2/DSI20-lessons/projects/project-4/stats_file.csv', sep=',', index_col=False) \n",
    "    results_unique_count = stats['Total number of unique results so far:'][0]\n",
    "    results_salaries_count = stats['Total number of salaries collected:'][0]\n",
    "    total_time = stats['Total time run for:'][0]\n",
    "    \n",
    "except: #otherwise just start again \n",
    "    j_title_count = 0\n",
    "    cities_count = 0\n",
    "    df_empty = pd.DataFrame(list())\n",
    "    df_empty.to_csv('results.csv')\n",
    "    df_empty.to_csv('stats_file.csv')\n",
    "    df_empty.to_csv('count_file.csv')\n",
    "    results_unique_count = 0\n",
    "    results_salaries_count = 0\n",
    "    start = time.time()\n",
    "    total_time = 0\n",
    "    \n",
    "    # Empty pandas DF and load empty to CSV -> THIS IS WHERE THE INFO IS STORED PRIOR TO BEING SAVED TO CSV\n",
    "    results = pd.DataFrame({'Location': location,\n",
    "                                              'Company': company,\n",
    "                                             'Job_Title': job_title, \n",
    "                                             'Salary': salary,\n",
    "                                             'Title_Searched': title_search,\n",
    "                                             'City_Searched': city_search,\n",
    "                                             'Link': link,\n",
    "                                             'Job Description': job_desc,\n",
    "                                             'Date Since Posted': date_since_post,\n",
    "                                             'Company Rating': company_rating})\n",
    "        \n",
    "    results_unique = results.drop_duplicates(keep='first', ignore_index=True, subset=['Company', 'Job_Title', 'Salary', 'Title_Searched', 'City_Searched'])   \n",
    "    results_unique.to_csv('results.csv', mode='a', header=True)\n",
    "\n",
    "########################## BELOW IS THE MAIN BODY OF CODE WHICH ITERATES THROUGH THE WEBPAGES AND SAVES RESULTS.    \n",
    "########################## THERE ARE 3 LOOPS TO ITERATE THROUGH JOB TITLES, CITY, EACH PAGE.  \n",
    "########################## AFTER ALL PAGES FOR A CITY HAVE BEEN SEARCHED FOR UNDER 1ST JOB TITLE, THE RESULTS ARE\n",
    "########################## FILTERED TO REMOVE DUPLICATES, SALARIES COUNTED THEN SAVED TO CSV.\n",
    "########################## IT THEM MOVES ONTO NEXT CITY. AFTER WHICH RESULTS ARE APPENDED TO ORIGINAL CSV\n",
    "########################## ONCE ALL CITIES ARE SEARCHED IT MOVES ONTO NEXT JOB TITLE E.G. DATA INTERN AND LOOPS\n",
    "########################## THROUGH ALL CITIES AGAIN. \n",
    "for j_title in job_titles_dscience2: \n",
    "#     print('Job title searched:',j_title)\n",
    "    \n",
    "    \n",
    "    for city in tqdm(cities4): # .loc[41747] = chicago\n",
    "        \n",
    "        city_searched_percentage = 100*cities_count/cities.shape[0]\n",
    " \n",
    "        pages_to_loop = pages_estimate(\"https://www.indeed.com/jobs?q=\" + j_title + \"&l=\" + city + \"&start=\" + '0' + \"%22&vjk=b9dedd85514851bc\")\n",
    "#         print('Pages to loop:', pages_to_loop)\n",
    "#         print(city, pages_to_loop)\n",
    "        for page in range(0,pages_to_loop,10):\n",
    "            url = \"https://www.indeed.com/jobs?q=\" + j_title + \"&l=\" + city + \"&start=\" + str(page) + \"&vjk=b9dedd85514851bc\"\n",
    "#             print(url)\n",
    "            r = requests.get(url, proxies=proxies, verify='/Users/stuart/Downloads/zyte-smartproxy-ca.crt')\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "            for i in soup.find_all('div', attrs={'class':'slider_container'}):\n",
    "                try: \n",
    "                    location.append(i.find('div', attrs={'class':'companyLocation'}).text)\n",
    "                except:\n",
    "                    location.append(np.nan)\n",
    "\n",
    "                try: \n",
    "                    company.append(i.find('span', attrs={'class':'companyName'}).text)\n",
    "                except:\n",
    "                    company.append(np.nan)\n",
    "\n",
    "                try: \n",
    "                    job_title.append(i.find('h2', attrs={'class':'jobTitle'}).text)\n",
    "                except:\n",
    "                    job_title.append(np.nan)\n",
    "\n",
    "                try: \n",
    "                    salary.append(i.find('div', attrs={'class':'metadata salary-snippet-container'}).text)\n",
    "                except:\n",
    "                    salary.append(np.nan)\n",
    "                    \n",
    "                try: \n",
    "                    job_desc.append(i.find('div', attrs={'class':'job-snippet'}).text)\n",
    "                except:\n",
    "                    job_desc.append(np.nan)\n",
    "                   \n",
    "                try: \n",
    "                    date_since_post.append(i.find('span', attrs={'class':'date'}).text)\n",
    "                except:\n",
    "                    date_since_post.append(np.nan)\n",
    "                \n",
    "                try: \n",
    "                    company_rating.append(i.find('span', attrs={'aria-hidden':'true'}).text)\n",
    "                except:\n",
    "                    company_rating.append(np.nan)\n",
    "                \n",
    "                \n",
    "                city_search.append(city)\n",
    "                title_search.append(j_title)\n",
    "                link.append(url)      \n",
    "                \n",
    "                \n",
    "        results = pd.DataFrame({'Location': location,\n",
    "                                             'Company': company,\n",
    "                                             'Job_Title': job_title, \n",
    "                                             'Salary': salary,\n",
    "                                             'Title_Searched': title_search,\n",
    "                                             'City_Searched': city_search,\n",
    "                                             'Link': link,\n",
    "                                             'Job Description': job_desc,\n",
    "                                             'Date Since Posted': date_since_post,\n",
    "                                             'Company Rating': company_rating})\n",
    "        \n",
    "        results_unique = results.drop_duplicates(keep='first', ignore_index=True, subset=['Company', 'Job_Title', 'Salary', 'Title_Searched', 'City_Searched'])\n",
    "        results_unique_count += results_unique.shape[0]\n",
    "        results_salaries_count += results_unique.Salary.notnull().sum()\n",
    "        \n",
    "########################## HERE RESULTS ARE FILTERED TO CHECK FOR UNIQUE VALUES IN THE CITY BEFORE BEING SAVED TO THE CSV\n",
    "########################## NOTE THIS WAS NOT PERFECT AS IT DOES NOT FILTER OUT REPEAT RESULTS ACCROSS DIFFERENT CITIES\n",
    "########################## E.G. IF THE SAME JOB CAME UP IN NEWYORK AND LA THE CSV WOULD STILL HAVE BOTH JOBS \n",
    "########################## THEREFORE THERE WOULD BE DUPLICATES\n",
    "########################## THIS WAS CLEANED UP LATER\n",
    "########################## NOTE THAT THIS WAS TERRIBLY INEFFICIENT AS I SCRAPED 300K URLS, ACCROSS 10% OF US CITIES\n",
    "########################## AND ONLY GOT 1000K SALARIES.\n",
    "########################## IT WOULD HAVE BEEN MUCH BETTER TO FOCUS ON THE LARGEST CITIES AND SECONDLY ON USING \n",
    "########################## DIFFERENT JOB TITLES\n",
    "\n",
    "        results_unique.to_csv('results.csv', mode='a', header=False)\n",
    "        \n",
    "        location = []\n",
    "        company = []\n",
    "        job_title = []\n",
    "        salary = []\n",
    "        city_search = []\n",
    "        title_search = []\n",
    "        link = []\n",
    "        job_desc = []\n",
    "        date_since_post = []\n",
    "        company_rating = []\n",
    "        \n",
    "        end = time.time()\n",
    "#         total_time += (end - start)\n",
    "    \n",
    "        stats = pd.DataFrame({'Total number of unique results so far:': results_unique_count,\n",
    "                              'Total number of salaries collected:': results_salaries_count,\n",
    "                              'Percentage of cities searched:':city_searched_percentage,\n",
    "                              'Total time run for:': total_time }, index = [0])\n",
    "        stats.to_csv('stats_file.csv')\n",
    "        \n",
    "        count_file = pd.DataFrame({'j_title1': j_title_count,\n",
    "                                             'cities_count1': cities_count}, index=[0])\n",
    "        count_file.to_csv('count_file.csv')\n",
    "        cities_count += 1\n",
    "    j_title_count += 1\n",
    "    cities_count = 0\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
